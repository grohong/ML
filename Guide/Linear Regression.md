# Linear Regression(선형 회기)

## 뉴런

- 인간의 **고차원 기능** 은 단순한 뉴련의 수많은 연결로 가능
- 뉴런 연결과 조정이 **학습**  
![neuron](/images/neuronImage.png)

- 이런 **뉴련** 의 동작을 아래와 같이 표현할수 있습니다.
![neuron](/images/neuron.png)

- **학습** 이란 위에 ```W(가중치)```를 조정하는 것(강하게, 혹은 약하게)


## 학습을 위한 용어

### hypothesis(가설)

- **hypothesis(가설)** : 증면되지 않았으나 조절을 통해 데이터를 잘 표현할 수 있는 것
- 뉴련의 출력을 식으로 표현
```python
h = Wx
h = Wx + b
# b가 있으면 뉴런은 더 다양한 분호(회기)를 표현할수 있습니다.
```
- 뉴련의 경우 **W** 값을 적적히 조절할 경우 **Linear Regression** 데이터를 잘 표현할 수 있음.

#### 어떻게 W를 조정할 것인가?

- 야단치면 된다.
- 다음에 좀 더 정답에 비슷하게 가도록 가중치(W)를 조정한다.
- 정답과 차이가 많이 난다? -> **에러비용(cost)** 이 크다.
- **에러비용(cost)** 가 0이 될때까지 **W** 와 **b** 를 조정한다.

****
### 오류 함수

- 오류(E) = (뉴련이 출력한값(가설) - 정답)^2
```python
E = (Wx - y)**2
```

- 만약, 데이터가 3개라면?

|x |y|
|-|-|
|1|1|
|2|2|
|3|3|

```python
E = tf.reduce_mean(tensorflow.square(Wx - y))
```

#### 오류 함수의 그래프

![costGraph](/images/costGraph.png)

- **w** 가 변하면 오류 **E** 도 변한다.
- **E** 를 줄이고 싶으면 **w** 를 적절히 바꾼다.
- **w** 위치에 따라 **w** 를 조금만 변경해도 **E** 가 크게 변한다. -> 정답과 멀다.
- **E** 가 적게 변한다. -> 정답과 가깝다.

#### 오류 함수가 변화(기울기) = 미치는 영향  
```python
W = W - a * 기울기 (a = 얼마나 반영할 지를 의미하는 상수)
```
